{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os, glob, random\n",
        "\n",
        "SOURCE = \"/content/drive/MyDrive/archive (1)/BreaKHis_v1/BreaKHis_v1/histology_slides/breast\"\n",
        "TARGET = \"/content/BreakHis_8class_fast\"\n",
        "\n",
        "subtypes = {\n",
        "    \"adenosis\": \"ADENOSIS\",\n",
        "    \"fibroadenoma\": \"FIBROADENOMA\",\n",
        "    \"phyllodes_tumor\": \"PHYLLODES_TUMOR\",\n",
        "    \"tubular_adenoma\": \"TUBULAR_ADENOMA\",\n",
        "    \"ductal_carcinoma\": \"DUCTAL_CARCINOMA\",\n",
        "    \"lobular_carcinoma\": \"LOBULAR_CARCINOMA\",\n",
        "    \"mucinous_carcinoma\": \"MUCINOUS_CARCINOMA\",\n",
        "    \"papillary_carcinoma\": \"PAPILLARY_CARCINOMA\"\n",
        "}\n",
        "\n",
        "splits = [\"train\", \"val\", \"test\"]\n",
        "for split in splits:\n",
        "    for label in subtypes.values():\n",
        "        os.makedirs(f\"{TARGET}/{split}/{label}\", exist_ok=True)\n",
        "\n",
        "exts = (\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\")\n",
        "subtype_files = {label: [] for label in subtypes.values()}\n",
        "\n",
        "for root, dirs, files in os.walk(SOURCE):\n",
        "    for file in files:\n",
        "        name = file.lower()\n",
        "        for key, label in subtypes.items():\n",
        "            if key in root.lower():\n",
        "                subtype_files[label].append(os.path.join(root, file))\n",
        "\n",
        "def fast_symlink(paths, cls):\n",
        "    random.shuffle(paths)\n",
        "    n = len(paths)\n",
        "    n_train = int(0.7 * n)\n",
        "    n_val = int(0.15 * n)\n",
        "\n",
        "    train, val, test = paths[:n_train], paths[n_train:n_train+n_val], paths[n_train+n_val:]\n",
        "\n",
        "    for f in train: os.symlink(f, f\"{TARGET}/train/{cls}/{os.path.basename(f)}\")\n",
        "    for f in val:   os.symlink(f, f\"{TARGET}/val/{cls}/{os.path.basename(f)}\")\n",
        "    for f in test:  os.symlink(f, f\"{TARGET}/test/{cls}/{os.path.basename(f)}\")\n",
        "\n",
        "    print(f\"{cls}: train={len(train)}, val={len(val)}, test={len(test)}\")\n",
        "\n",
        "for cls, flist in subtype_files.items():\n",
        "    fast_symlink(flist, cls)\n",
        "\n",
        "print(\"\\n✅ 8-class dataset ready at:\", TARGET)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuEguP35C-mI",
        "outputId": "a9c9fdca-6d6d-4ce6-846a-deacde17b4a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ADENOSIS: train=310, val=66, test=68\n",
            "FIBROADENOMA: train=709, val=152, test=153\n",
            "PHYLLODES_TUMOR: train=317, val=67, test=69\n",
            "TUBULAR_ADENOMA: train=398, val=85, test=86\n",
            "DUCTAL_CARCINOMA: train=2415, val=517, test=519\n",
            "LOBULAR_CARCINOMA: train=438, val=93, test=95\n",
            "MUCINOUS_CARCINOMA: train=554, val=118, test=120\n",
            "PAPILLARY_CARCINOMA: train=392, val=84, test=84\n",
            "\n",
            "✅ 8-class dataset ready at: /content/BreakHis_8class_fast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "\n",
        "TARGET = \"/content/BreakHis_8class_fast\"\n",
        "\n",
        "if os.path.exists(TARGET):\n",
        "    shutil.rmtree(TARGET)\n",
        "\n",
        "print(\"Old dataset removed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq_8NmZMuX_-",
        "outputId": "c6f779bf-6591-4d8d-eca9-04ef618b7bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old dataset removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os, random, copy\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "DATA_ROOT   = \"/content/BreakHis_8class_fast\"\n",
        "IMG_SIZE    = 456\n",
        "BATCH_SIZE  = 24\n",
        "EPOCHS      = 20\n",
        "BASE_LR     = 8e-4\n",
        "PATIENCE    = 5\n",
        "NUM_WORKERS = 2\n",
        "SEED        = 42\n",
        "\n",
        "USE_EMA        = True\n",
        "MIXUP_ALPHA    = 0.15\n",
        "LABEL_SMOOTH   = 0.03\n",
        "USE_TTA        = True\n",
        "MAX_GRAD_NORM  = 3.0\n",
        "\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "non_block = torch.cuda.is_available()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.75, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(0.25, 0.25, 0.25),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "train_set = datasets.ImageFolder(f\"{DATA_ROOT}/train\", transform=train_tf)\n",
        "val_set   = datasets.ImageFolder(f\"{DATA_ROOT}/val\",   transform=test_tf)\n",
        "test_set  = datasets.ImageFolder(f\"{DATA_ROOT}/test\",  transform=test_tf)\n",
        "\n",
        "classes = train_set.classes\n",
        "num_classes = len(classes)\n",
        "\n",
        "g = torch.Generator().manual_seed(SEED)\n",
        "pin = torch.cuda.is_available()\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, pin_memory=pin,\n",
        "    persistent_workers=NUM_WORKERS > 0, generator=g\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_set, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=pin,\n",
        "    persistent_workers=NUM_WORKERS > 0\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_set, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=pin,\n",
        "    persistent_workers=NUM_WORKERS > 0\n",
        ")\n",
        "\n",
        "\n",
        "counts = np.zeros(num_classes, dtype=np.int64)\n",
        "for _, y in train_set.samples:\n",
        "    counts[y] += 1\n",
        "\n",
        "w = 1.0 / np.maximum(counts, 1)\n",
        "w = w / w.sum() * num_classes\n",
        "class_weights = torch.tensor(w, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "model = models.efficientnet_b5(\n",
        "    weights=models.EfficientNet_B5_Weights.IMAGENET1K_V1\n",
        ")\n",
        "\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "for p in model.features[-1].parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "in_feats = model.classifier[1].in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(in_feats, 512),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.35),\n",
        "    nn.Linear(512, num_classes)\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(memory_format=torch.channels_last)\n",
        "\n",
        "\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.model = copy.deepcopy(model).eval()\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        for ema_p, p in zip(self.model.parameters(), model.parameters()):\n",
        "            ema_p.data.mul_(self.decay).add_(p.data, alpha=1 - self.decay)\n",
        "\n",
        "ema = EMA(model) if USE_EMA else None\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    weight=class_weights,\n",
        "    label_smoothing=LABEL_SMOOTH\n",
        ")\n",
        "\n",
        "params = list(model.classifier.parameters()) + \\\n",
        "         list(model.features[-1].parameters())\n",
        "\n",
        "optimizer = optim.AdamW(params, lr=BASE_LR, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=4)\n",
        "\n",
        "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "def mixup(x, y, alpha):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    idx = torch.randperm(x.size(0), device=x.device)\n",
        "    return lam * x + (1 - lam) * x[idx], (y, y[idx]), lam\n",
        "\n",
        "def mixup_loss(pred, y, lam):\n",
        "    y1, y2 = y\n",
        "    return lam * criterion(pred, y1) + (1 - lam) * criterion(pred, y2)\n",
        "\n",
        "\n",
        "def evaluate(loader, model):\n",
        "    model.eval()\n",
        "    total, correct, n = 0.0, 0, 0\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                x = x.to(memory_format=torch.channels_last)\n",
        "\n",
        "            with autocast(\"cuda\", enabled=torch.cuda.is_available()):\n",
        "                out = model(x)\n",
        "                loss = criterion(out, y)\n",
        "\n",
        "            total += loss.item()\n",
        "            pred = out.argmax(1)\n",
        "\n",
        "            correct += (pred == y).sum().item()\n",
        "            n += y.size(0)\n",
        "            y_true.extend(y.cpu().tolist())\n",
        "            y_pred.extend(pred.cpu().tolist())\n",
        "\n",
        "    return (\n",
        "        total / len(loader),\n",
        "        correct / n,\n",
        "        f1_score(y_true, y_pred, average=\"macro\"),\n",
        "        y_true,\n",
        "        y_pred\n",
        "    )\n",
        "\n",
        "best_f1 = -1.0\n",
        "patience = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    for step, (x, y) in enumerate(pbar):\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.to(memory_format=torch.channels_last)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast(\"cuda\", enabled=torch.cuda.is_available()):\n",
        "            if MIXUP_ALPHA > 0:\n",
        "                xm, ym, lam = mixup(x, y, MIXUP_ALPHA)\n",
        "                out = model(xm)\n",
        "                loss = mixup_loss(out, ym, lam)\n",
        "            else:\n",
        "                out = model(x)\n",
        "                loss = criterion(out, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(params, MAX_GRAD_NORM)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if USE_EMA:\n",
        "            ema.update(model)\n",
        "\n",
        "        scheduler.step(epoch + step / len(train_loader))\n",
        "        running += loss.item()\n",
        "        pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "    train_loss = running / len(train_loader)\n",
        "    val_loss, val_acc, val_f1, _, _ = evaluate(\n",
        "        val_loader, ema.model if USE_EMA else model\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Train {train_loss:.4f} | \"\n",
        "        f\"Val {val_loss:.4f} | \"\n",
        "        f\"Acc {val_acc:.4f} | \"\n",
        "        f\"F1 {val_f1:.4f}\"\n",
        "    )\n",
        "\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        patience = 0\n",
        "        torch.save(\n",
        "            (ema.model if USE_EMA else model).state_dict(),\n",
        "            \"best_breakhis_effb5_fast.pth\"\n",
        "        )\n",
        "        print(\"✓ Saved best model\")\n",
        "    else:\n",
        "        patience += 1\n",
        "        if patience >= PATIENCE:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "\n",
        "ckpt = torch.load(\"best_breakhis_effb5_fast.pth\", map_location=device)\n",
        "\n",
        "if USE_EMA:\n",
        "    ema.model.load_state_dict(ckpt)\n",
        "    test_model = ema.model\n",
        "else:\n",
        "    model.load_state_dict(ckpt)\n",
        "    test_model = model\n",
        "\n",
        "test_loss, test_acc, test_f1, y_true, y_pred = evaluate(test_loader, test_model)\n",
        "\n",
        "print(f\"\\nTest | Loss {test_loss:.4f} | Acc {test_acc:.4f} | F1 {test_f1:.4f}\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\",\n",
        "            xticklabels=classes, yticklabels=classes,\n",
        "            cmap=\"Blues\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "tta_transforms = [\n",
        "    test_tf,\n",
        "    transforms.Compose([transforms.RandomHorizontalFlip(p=1.0), *test_tf.transforms]),\n",
        "    transforms.Compose([transforms.RandomVerticalFlip(p=1.0), *test_tf.transforms]),\n",
        "]\n",
        "\n",
        "def predict_image(path):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    test_model.eval()\n",
        "    probs = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for tfm in tta_transforms:\n",
        "            x = tfm(img).unsqueeze(0).to(device)\n",
        "            if torch.cuda.is_available():\n",
        "                x = x.to(memory_format=torch.channels_last)\n",
        "            probs += torch.softmax(test_model(x), dim=1)\n",
        "\n",
        "    probs /= len(tta_transforms)\n",
        "    return classes[probs.argmax(1).item()]\n",
        "\n",
        "\n",
        "IMNET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
        "IMNET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
        "\n",
        "def denorm(x):\n",
        "    return (x * IMNET_STD + IMNET_MEAN).clamp(0,1)\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, layer):\n",
        "        self.grad = None\n",
        "        self.act = None\n",
        "        layer.register_forward_hook(self._fwd)\n",
        "        layer.register_backward_hook(self._bwd)  # safer fallback\n",
        "\n",
        "    def _fwd(self, m, i, o):\n",
        "        self.act = o\n",
        "\n",
        "    def _bwd(self, m, gi, go):\n",
        "        self.grad = go[0]\n",
        "\n",
        "    def generate(self, idx=0):\n",
        "        w = self.grad[idx].mean(dim=(1,2), keepdim=True)\n",
        "        cam = (w * self.act[idx]).sum(0).clamp(min=0)\n",
        "        cam /= cam.max() + 1e-8\n",
        "        return cam.cpu().numpy()\n",
        "\n",
        "target_layer = test_model.features[-1]\n",
        "cam = GradCAM(test_model, target_layer)\n",
        "\n",
        "img_tensor, _ = test_set[0]\n",
        "x = img_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "test_model.zero_grad()\n",
        "out = test_model(x)\n",
        "pred = out.argmax(1).item()\n",
        "out[0, pred].backward()\n",
        "\n",
        "heat = cam.generate(0)\n",
        "\n",
        "plt.imshow(denorm(img_tensor).permute(1,2,0))\n",
        "plt.imshow(heat, cmap=\"jet\", alpha=0.45)\n",
        "plt.title(f\"Grad-CAM: {classes[pred]}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nKfF0sdmUyNV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68fe70ec-4115-458e-81d4-687f8acf813b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   1%|▏         | 3/231 [01:44<2:10:46, 34.41s/it, loss=2.13]"
          ]
        }
      ]
    }
  ]
}